"""
Read corpora that used in experiments.
"""

# STD
from typing import List


def read_gulordava_corpus(corpus_dir: str) -> dict:
    """
    Read the corpus generated by [1] with 41 sentences, of which all content words were randomly replaced with another
    word of the same form nine time, resulting in 410 sentences in total.

    [1] https://arxiv.org/pdf/1803.11138.pdf

    Parameters
    ----------
    corpus_dir: str
        Directory to corpus files.

    Returns
    -------
    labelled_corpus: dict
        Corpus of labelled sentences as a dictionary from sentence id as key to LabelledSentence object as value.
    """
    def _read_file(path: str) -> List[str]:
        with open(path, "r") as f:
            return f.readlines()

    sentences = _read_file(f"{corpus_dir}/generated.text")
    sentence_info = _read_file(f"{corpus_dir}/generated.tab")[1:]  # Skip header line
    labelled_corpus = {}

    for i, sentence in enumerate(sentences):
        right_info, wrong_info = sentence_info[2*i], sentence_info[2*i+1]

        # Parse lines
        right_info, wrong_info = right_info.split("\t"), wrong_info.split("\t")
        constr_id, sent_id, correct_number, right_form, class_, type_ = right_info[1:7]
        len_context, len_prefix, sent = right_info[11:14]
        constr_id_wrong, sent_id_wrong, _, wrong_form, class_wrong, type_wrong = wrong_info[1:7]
        sent_wrong = wrong_info[13]

        assert class_ == "correct" and class_wrong == "wrong"
        assert constr_id == constr_id_wrong and sent_id == sent_id_wrong and sent == sent_wrong and type_ == type_wrong

        len_prefix, len_context = int(len_prefix), int(len_context)
        subj_pos = len_prefix - len_context
        verb_pos = len_prefix
        sentence = sent.split()

        misc_info = {
            "raw": sent,
            "subj_pos": subj_pos,
            "verb_pos": verb_pos,
            "right_form": right_form,
            "wrong_form": wrong_form,
            "correct_number": correct_number,
            "sent_id": sent_id,
            "constr_id": constr_id,
            "type": type_
        }

        labelled_sentence = {
            "sen": sentence, "labels": [0 if correct_number == "sing" else 1] * len(sentence), **misc_info
        }
        labelled_corpus[i] = labelled_sentence

    return labelled_corpus


def read_giulianelli_corpus(corpus_path: str) -> dict:
    """
    Read the corpus created in [1] with contains sentences from the Wikidata corpus used in [2], filtered by the
    number of words preceeding the subject, the number of words following the main verb, the context size and the number
    of helpful nouns or attractor between subject and main verb.

    [1] http://aclweb.org/anthology/W18-5426
    [2] https://transacl.org/ojs/index.php/tacl/article/download/972/215
    """
    labelled_corpus = {}

    with open(corpus_path, "r") as corpus_file:
        for i, line in enumerate(corpus_file.readlines()):
            line = line.strip()

            # Parse line
            raw_sentence, raw_sentence_pos, label, subj_pos, verb_pos, num_helpful, num_attractors = line.split("\t")
            sentence, sentence_pos = raw_sentence.split(), raw_sentence_pos.split()
            subj_pos, verb_pos, num_helpful, num_attractors = map(int, [subj_pos, verb_pos, num_helpful, num_attractors])

            # Organize information
            misc_info = {
                "raw": raw_sentence,
                "subj_pos": subj_pos,
                "verb_pos": verb_pos,
                "pos_tags": sentence_pos,
                "num_helpful": num_helpful,
                "num_attractors": num_attractors
            }

            labelled_sentence = {
                "sen": sentence, "labels": [0 if label == "sing" else 1] * len(sentence), **misc_info
            }
            labelled_corpus[i] = labelled_sentence

    return labelled_corpus
